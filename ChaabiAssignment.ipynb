{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CHAABI ASSIGNMENT: BUILDING A QUERY ENGINE FROM A GIVEN DATA SOURCE\n",
        "\n",
        "##### By Abhranil Das, d.abhranil@iitg.ac.in\n",
        "##### Roll no. 200108002, Electronics and Electrical Engineering, IIT Guwahati\n"
      ],
      "metadata": {
        "id": "BGinxZm5Rk8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "ChatGPT and other similar models struggle with generating factual statements if no context is provided. They have some general knowledge but cannot guarantee to produce a valid answer consistently. Thus, it is better to provide some facts we know are actual, so it can just choose the valid parts and extract them from all the provided contextual data to give a comprehensive answer. Vector databases, such as FAISS, Qdrant, Pinecone, .etc, can be of great help here, as their ability to perform a semantic search over a huge knowledge base is crucial to preselect some possibly valid documents, so they can be provided into the LLM, which then answers queries based on the context extracted from the queries.\n",
        "\n",
        "## What do we need?\n",
        "We need two models to set up a query engine using any LLM. First of all, we need an embedding model that will convert the set of facts into vectors, and will store it in a vector database like Qdrant or FAISS.  We’re going to use one of the Hugging Face Instruct Embedding models, so it can be hosted locally. The embeddings created by that model will be put into  and used to retrieve the most similar documents, given the query.\n",
        "\n",
        "However, when we receive a query, there are two steps involved. First of all, we ask the vector database to provide the most relevant documents and simply combine all of them into a single text. Then, we build a prompt to the LLM, including those documents as a context, of course together with the question asked. So the input to the LLM looks like the following:\n",
        "\n",
        "\n",
        "\n",
        "> Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \\\n",
        "\\\n",
        "> ...  \n",
        "It's as certain as 2 + 2 = 4  \n",
        "...\n",
        "\\\n",
        "\\\n",
        "> Question: How much is 2 + 2? \\\n",
        "Helpful Answer:\n",
        "\n",
        "\n",
        "\n",
        "There might be several context documents combined, and it is solely up to LLM to choose the right piece of content. But our expectation is, the model should respond with just 4.\n",
        "\n",
        "Why do we need two different models? Both solve some different tasks. The first model performs feature extraction, by converting the text into vectors, while the second one helps in text generation or summarization. Disclaimer: This is not the only way to solve that task with LangChain. Such a chain is called stuff in the library nomenclature.\n",
        "\n",
        "This is called **Retrieval Augmented Generation (RAG)**. The pipeline for RAG looks like the following:\n",
        "\n",
        "<a href=\"https://ibb.co/7yYLxsS\"><img src=\"https://i.ibb.co/x5CdKx2/Screenshot-217.png\" alt=\"Screenshot-217\" border=\"0\" /></a>\n",
        "\n",
        "We will be using **Langchain** to build the query engine, starting from generating the embeddings, building the vector database to implementing the LLM and creating a query-answer interface. Every step is documented and explained in details in this notebook.\n",
        "\n",
        "## Langchain:\n",
        "\n",
        "LangChain provides unified interfaces to different libraries, so one can avoid writing boilerplate code and focus on the value he/she wants to bring. Langchain supports various pre-trained models for generating vector embeddings, and also supports popular vector databases like Qdrant, Pinecone, FAISS, .etc, which can be used for storing the embeddings. LangChain also allows us to utilize already pre-trained models and support even complex pipelines with a few lines of code, making the process of building applications with LLMs (Large-Language Models) efficient and simple.\n",
        "\n",
        "<img src = \"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-PlFCd_VBcALKReO3ZaOEg.png\"/>"
      ],
      "metadata": {
        "id": "H7nRmmZZhGZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we list out all the steps we need to create a Query Engine, given any data source/database. We will be using Hugging Face Instruct Embeddings (hkunlp/instructor-base) model to generate the embeddings, FAISS to create the vector database, Hugging Face Hub as the LLM and Retrieval QA to build the engine."
      ],
      "metadata": {
        "id": "EovzagpIn3p8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INITIAL SANITY CHECK - INSTALLING DEPENDENCIES AND IMPORTING NECESSARY LIBRARIES"
      ],
      "metadata": {
        "id": "Wihw4zQjY67i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to install all the dependencies that are needed to run this notebook"
      ],
      "metadata": {
        "id": "2i5cd_76eYc7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1IzFJrJBdnG"
      },
      "outputs": [],
      "source": [
        "!pip install langchain InstructorEmbedding sentence_transformers faiss-gpu huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to import the necessary modules"
      ],
      "metadata": {
        "id": "SELP7hoyeoVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "import sentence_transformers\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "import os, sys, warnings\n",
        "from getpass import getpass\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "lzukM9HAYaas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UPLOADING THE DATA SOURCE"
      ],
      "metadata": {
        "id": "U3nGAawdZCJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to input the data source to be used for building the query engine. After running the cell, click the **Choose files** button and upload the dataset file from your computer. After uploading, wait for the file to be fully uploaded, then run the next cell."
      ],
      "metadata": {
        "id": "ASr_lozCevLC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "hjNYUJHtASNP",
        "outputId": "ea192cbc-d6d8-42d5-85e4-b784830e73b5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bda77d55-b23b-452d-846f-4225f6ab5608\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bda77d55-b23b-452d-846f-4225f6ab5608\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving bigBasketProducts.csv to bigBasketProducts (1).csv\n"
          ]
        }
      ],
      "source": [
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = list(uploaded.keys())[0]"
      ],
      "metadata": {
        "id": "sZgYbr6DReaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PREPARING THE DATALOADER AND VISUALIZING THE DATA"
      ],
      "metadata": {
        "id": "m0rUyW8DZFma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using the CSVLoader function from Langchain to build the database from the CSV file provided as input. We also use Pandas to load the first few rows and visualize them to see how our database looks like. Run the following two cells to prepare the data to be used as the database for the engine."
      ],
      "metadata": {
        "id": "Csg3VErYorOm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bpEtz1WBwGm"
      },
      "outputs": [],
      "source": [
        "loader = CSVLoader(file_path = filename, encoding = 'utf-8')\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "GgJ8_UXICMbi",
        "outputId": "ca5f1ed6-43cf-4ddd-b93d-d8844c0af605"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 product  \\\n",
              "index                                                      \n",
              "1                 Garlic Oil - Vegetarian Capsule 500 mg   \n",
              "2                                  Water Bottle - Orange   \n",
              "3                         Brass Angle Deep - Plain, No.2   \n",
              "4      Cereal Flip Lid Container/Storage Jar - Assort...   \n",
              "5                     Creme Soft Soap - For Hands & Body   \n",
              "\n",
              "                     category           sub_category              brand  \\\n",
              "index                                                                     \n",
              "1            Beauty & Hygiene              Hair Care  Sri Sri Ayurveda    \n",
              "2      Kitchen, Garden & Pets  Storage & Accessories         Mastercook   \n",
              "3        Cleaning & Household            Pooja Needs                Trm   \n",
              "4        Cleaning & Household   Bins & Bathroom Ware             Nakoda   \n",
              "5            Beauty & Hygiene       Bath & Hand Wash              Nivea   \n",
              "\n",
              "       sale_price  market_price                      type  rating  \\\n",
              "index                                                               \n",
              "1           220.0           220          Hair Oil & Serum     4.1   \n",
              "2           180.0           180    Water & Fridge Bottles     2.3   \n",
              "3           119.0           250           Lamp & Lamp Oil     3.4   \n",
              "4           149.0           176  Laundry, Storage Baskets     3.7   \n",
              "5           162.0           162      Bathing Bars & Soaps     4.4   \n",
              "\n",
              "                                             description  \n",
              "index                                                     \n",
              "1      This Product contains Garlic Oil that is known...  \n",
              "2      Each product is microwave safe (without lid), ...  \n",
              "3      A perfect gift for all occasions, be it your m...  \n",
              "4      Multipurpose container with an attractive desi...  \n",
              "5      Nivea Creme Soft Soap gives your skin the best...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a8a065d1-0447-4014-b766-b18c597f11fa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>product</th>\n",
              "      <th>category</th>\n",
              "      <th>sub_category</th>\n",
              "      <th>brand</th>\n",
              "      <th>sale_price</th>\n",
              "      <th>market_price</th>\n",
              "      <th>type</th>\n",
              "      <th>rating</th>\n",
              "      <th>description</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Garlic Oil - Vegetarian Capsule 500 mg</td>\n",
              "      <td>Beauty &amp; Hygiene</td>\n",
              "      <td>Hair Care</td>\n",
              "      <td>Sri Sri Ayurveda</td>\n",
              "      <td>220.0</td>\n",
              "      <td>220</td>\n",
              "      <td>Hair Oil &amp; Serum</td>\n",
              "      <td>4.1</td>\n",
              "      <td>This Product contains Garlic Oil that is known...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Water Bottle - Orange</td>\n",
              "      <td>Kitchen, Garden &amp; Pets</td>\n",
              "      <td>Storage &amp; Accessories</td>\n",
              "      <td>Mastercook</td>\n",
              "      <td>180.0</td>\n",
              "      <td>180</td>\n",
              "      <td>Water &amp; Fridge Bottles</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Each product is microwave safe (without lid), ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Brass Angle Deep - Plain, No.2</td>\n",
              "      <td>Cleaning &amp; Household</td>\n",
              "      <td>Pooja Needs</td>\n",
              "      <td>Trm</td>\n",
              "      <td>119.0</td>\n",
              "      <td>250</td>\n",
              "      <td>Lamp &amp; Lamp Oil</td>\n",
              "      <td>3.4</td>\n",
              "      <td>A perfect gift for all occasions, be it your m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Cereal Flip Lid Container/Storage Jar - Assort...</td>\n",
              "      <td>Cleaning &amp; Household</td>\n",
              "      <td>Bins &amp; Bathroom Ware</td>\n",
              "      <td>Nakoda</td>\n",
              "      <td>149.0</td>\n",
              "      <td>176</td>\n",
              "      <td>Laundry, Storage Baskets</td>\n",
              "      <td>3.7</td>\n",
              "      <td>Multipurpose container with an attractive desi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Creme Soft Soap - For Hands &amp; Body</td>\n",
              "      <td>Beauty &amp; Hygiene</td>\n",
              "      <td>Bath &amp; Hand Wash</td>\n",
              "      <td>Nivea</td>\n",
              "      <td>162.0</td>\n",
              "      <td>162</td>\n",
              "      <td>Bathing Bars &amp; Soaps</td>\n",
              "      <td>4.4</td>\n",
              "      <td>Nivea Creme Soft Soap gives your skin the best...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a8a065d1-0447-4014-b766-b18c597f11fa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a8a065d1-0447-4014-b766-b18c597f11fa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a8a065d1-0447-4014-b766-b18c597f11fa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ca767e02-690b-4c65-a4d9-2d66dcd1557a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ca767e02-690b-4c65-a4d9-2d66dcd1557a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ca767e02-690b-4c65-a4d9-2d66dcd1557a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "df = pd.read_csv(filename, nrows = 20, encoding = 'utf-8', encoding_errors = 'ignore', index_col = [0])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CREATING VECTOR EMBEDDINGS"
      ],
      "metadata": {
        "id": "BfzxEFkXZLRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After creating the database, the next step is to generate vector embeddings from it. For this we we will be using the **hku-nlp/instructor-base** model to generate the embeddings. Model details and implementation can be found here [*hku-nlp/instructor-base*](https://huggingface.co/hku-nlp/instructor-base)."
      ],
      "metadata": {
        "id": "ilH12zPxpItY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdwTrzJtDx5m",
        "outputId": "86f02b0d-812a-4bf2-92c7-75ce1ad4e4fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ],
      "source": [
        "embeddings = HuggingFaceInstructEmbeddings(model_name = 'hku-nlp/instructor-base', model_kwargs = {\"device\":  \"cuda\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STORING THE EMBEDDINGS IN A VECTOR DATABASE"
      ],
      "metadata": {
        "id": "I-I28x_RZP7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to define a vector database which will store the embeddings generated from the dataset. For this we will be using **FAISS**.\n",
        "\n",
        "**Facebook AI Similarity Search (FAISS)**  is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning."
      ],
      "metadata": {
        "id": "lqmwxyc_slXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncomment the following cell and run it to create the vector store that contains the embeddings generated from the database. This takes a lot of time to run, so we store the vector store in a local folder so that we can reuse the embeddings later without having to generate them again."
      ],
      "metadata": {
        "id": "hguPVqCfuQ23"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Y_b9OWNEfhd"
      },
      "outputs": [],
      "source": [
        "# vectorstore = FAISS.from_documents(data, embeddings)\n",
        "# vectorstore.save_local('FAISS_Index')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After saving the vector store, run the following cell to load the vector store from the saved folder."
      ],
      "metadata": {
        "id": "bDqv6Y0xuoW-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDTncjHSGRDI"
      },
      "outputs": [],
      "source": [
        "vectorstore = FAISS.load_local('FAISS_Index', embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell implements one of the functionalities of FAISS (and for that matter, any vector database) which is **similarity search**. Given any query, similarity search compares the vectors stored in the database and find the ones that are most similar to the query vector. This is important, as similarity search extracts the documents that are the most relevant to the query inputted by the user (the documents are then later fed to the LLM).\n",
        "\n",
        "We illustrate it with a simple example."
      ],
      "metadata": {
        "id": "Idmk3alqu64t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = vectorstore.similarity_search_with_score(\"Best oil for cooking\")\n",
        "docs = sorted(docs, key = lambda x: x[1], reverse = True)\n",
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UVhQFwS-J2O",
        "outputId": "aec12cdc-e4a3-4f9d-ab4b-a47bbea12014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Document(page_content='index: 26556\\nproduct: Extra Light Olive Oil\\ncategory: Foodgrains, Oil & Masala\\nsub_category: Edible Oils & Ghee\\nbrand: Jivo\\nsale_price: 1714\\nmarket_price: 3900\\ntype: Olive & Canola Oils\\nrating: 3.3\\ndescription: Extra Light Olive Oil is suitable for all types of Indian cuisine and deep-frying. It may help in lowering bad cholesterol, prevents strokes, protects heart disease, and fights Alzheimer disease. Also, it benefits to heart, brain, joints and more.', metadata={'source': 'bigBasketProducts.csv', 'row': 26555}),\n",
              " 0.20270832)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPLEMENTING THE LARGE LANGUAGE MODEL (LLM)"
      ],
      "metadata": {
        "id": "W6FG6_irZZZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step, and possibly the most important step in building the query engine, is implementing the Large Language Model (LLM). The LLM plays the pivotal role in the search engine: it receives the documents relevant to the query from the vector database, analyzes the documents for extracting query context, and then tries to answer the query on the basis of the context extracted from the query. In short, LLM act as the interface between the user and the database: it receives the query from the user, and provides the answer from the database.\n",
        "\n",
        "Here we will be using the **Hugging Face Hub** to implement our LLM. The Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together."
      ],
      "metadata": {
        "id": "bHgETEWHvvpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement the LLM, we first need an API token from Huggingface. We can obtain the API token by going to [Get Your API Token](https://huggingface.co/docs/api-inference/quicktour#get-your-api-token), and following the steps mentioned there:\n",
        "1. Login to or create a new account at Hugging face\n",
        "2. Go to **Hugging Face profile settings** and get the API token/User access from there\n",
        "\n",
        "After obtaining the Hugging Face API token, run the following cell and enter the token, which will be used by the Hugging Face Hub LLM."
      ],
      "metadata": {
        "id": "OsVk80xxzJNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw1uOJ0BEwLG",
        "outputId": "5a7fd406-37d9-4b5d-c62d-1d6d0a00b48b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face Hub has a lot of models one can access to build a LLM. We will be using **Flan** by Google as our LLM.\n",
        "\n",
        "For other options, visit this website https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads"
      ],
      "metadata": {
        "id": "1yzyjh-t0TOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [StreamingStdOutCallbackHandler()]\n",
        "\n",
        "llm = HuggingFaceHub(repo_id = \"google/flan-t5-xxl\", model_kwargs = {\"temperature\": 0.5, \"max_length\": 64})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NS4Uuap_9od",
        "outputId": "f603ef74-f72a-4c3f-f97f-d6ec949a93e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BUILDING THE QUERY-ANSWER INTERFACE USING RETRIEVAL-AUGMENTED GENERATION"
      ],
      "metadata": {
        "id": "p54_tuJ6ZfnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have defined the model for generating embeddings, the vector store/database, and the LLM, the final step is to combine all these three into a RAG-based query-answer chain, which takes the query from the user, generates its embedding, obtains the document vectors most similar to the query vector from the vector store, passes those document vectors to the LLM, obtains the answer from the LLM and passes it to the user.\n",
        "\n",
        "We use the **RetrievalQA** method of LangChain to build our query-answer chain. We pass the LLM that we implemented in the previous step, and the vector store retriever (the method of the vector store that retrieves documents most similar to the query) as arguments to the method. ```code_type = stuff``` means that there exists multiple chains with which we can retrieve the answer to a question. This chain is one of the possible ways, and is called **stuff** in library nomenclature.\n",
        "\n",
        "Once the question-answer chain is created, we pass a few sample questions as queries to the chain, and print the answers. Run the following cells to see the output for the same.\n",
        "\n"
      ],
      "metadata": {
        "id": "RhqA81mPTYbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(llm = llm, chain_type = 'stuff', retriever = vectorstore.as_retriever(), callbacks = callbacks)"
      ],
      "metadata": {
        "id": "ZsLGKpuEAE9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = ['Which company claims to have the best canola oil?',\n",
        "             'What is the best type of cooking oil?',\n",
        "             'Which brand has the best rated shampoo?',\n",
        "             'Which brand has the best cooking oil under 500 Rs?',\n",
        "             'Name a product made by Sri Sri Ayurveda']"
      ],
      "metadata": {
        "id": "jsIOwqABALGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for question in questions:\n",
        "  print(f\"Question: {question}\")\n",
        "  print(f\"Answer: {qa.run(question)}\", end = \"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euxy8lwsCog7",
        "outputId": "3883d06e-b375-4f4f-e025-312e3bc20c19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Which company claims to have the best canola oil?\n",
            "Answer: Disano\n",
            "\n",
            "Question: What is the best type of cooking oil?\n",
            "Answer: Canola Oil\n",
            "\n",
            "Question: Which brand has the best rated shampoo?\n",
            "Answer: Nyle\n",
            "\n",
            "Question: Which brand has the best cooking oil under 500 Rs?\n",
            "Answer: Earthon\n",
            "\n",
            "Question: Name a product made by Sri Sri Ayurveda\n",
            "Answer: Pradara Shamaka Syrup\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eY7OKycUaHXC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}